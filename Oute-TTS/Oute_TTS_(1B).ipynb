{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AK-Github-0/TTS-Finetuning-for-Arabic/blob/main/Oute-TTS/Oute_TTS_(1B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlrBSHsOgp_9"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNAHuew6gp__"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu0lCFXhgqAA"
      },
      "source": [
        "Read our **[TTS Guide](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning)** for instructions and all our notebooks.\n",
        "\n",
        "Read our **[Qwen3 Guide](https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luMGktoxgqAB"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-As8-QGgqAB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "\n",
        "# Extra installations for Oute\n",
        "!pip install omegaconf einx\n",
        "!git clone https://github.com/edwko/OuteTTS\n",
        "import os\n",
        "os.remove(\"/content/OuteTTS/outetts/models/gguf_model.py\")\n",
        "os.remove(\"/content/OuteTTS/outetts/interface.py\")\n",
        "os.remove(\"/content/OuteTTS/outetts/__init__.py\")\n",
        "!pip install pyloudnorm openai-whisper uroman MeCab loguru flatten_dict ffmpy randomname argbind tiktoken\n",
        "!pip install descript-audio-codec descript-audiotools julius openai-whisper --no-deps\n",
        "%env UNSLOTH_DISABLE_FAST_GENERATION = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkWYsztAs9Ky"
      },
      "source": [
        "### Unsloth\n",
        "\n",
        "`FastModel` supports loading nearly any model now! This includes Vision and Text models!\n",
        "\n",
        "Thank you to [Etherl](https://huggingface.co/Etherll) for creating this notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any for long context!\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "    # Qwen3 new models\n",
        "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
        "    # Other very popular models!\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/Llama-3.3-70B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-OuteTTS-1.0-1B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = None, # Set to None for auto detection\n",
        "    load_in_4bit = False, # Set to True for 4bit which reduces memory\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"v_proj\",],\n",
        "    lora_alpha = 128,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep  \n",
        "\n",
        "We will use the `MrDragonFox/Elise`, which is designed for training TTS models. Ensure that your dataset follows the required format: **text, audio**, but maintaining the correct structure is essential for optimal training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset,Audio,Dataset\n",
        "dataset = load_dataset(\"MohamedRashad/SCC22\", split = \"test\")\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=24000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zK94B-Pfioto"
      },
      "outputs": [],
      "source": [
        "#@title Tokenization Function\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import io\n",
        "import tempfile\n",
        "from datasets import Dataset\n",
        "import sys\n",
        "sys.path.append('OuteTTS')\n",
        "import os\n",
        "import dac\n",
        "# V3 Imports\n",
        "from outetts.version.v3.audio_processor import AudioProcessor\n",
        "from outetts.version.v3.prompt_processor import PromptProcessor\n",
        "from outetts.dac.interface import DacInterface\n",
        "from outetts.models.config import ModelConfig # Need a dummy config for AudioProcessor\n",
        "import whisper\n",
        "from outetts.utils.preprocessing import text_normalizations\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "\n",
        "class DataCreationV3:\n",
        "    def __init__(\n",
        "            self,\n",
        "            model_tokenizer_path: str,\n",
        "            whisper_model_name: str = \"turbo\",\n",
        "            device: str = None\n",
        "        ):\n",
        "\n",
        "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Create a dummy ModelConfig mainly for device and paths needed by AudioProcessor/DacInterface\n",
        "        dummy_config = ModelConfig(\n",
        "            tokenizer_path=model_tokenizer_path,\n",
        "            device=self.device,\n",
        "            audio_codec_path=None # Let AudioProcessor use default DAC path\n",
        "        )\n",
        "        self.audio_processor = AudioProcessor(config=dummy_config)\n",
        "        self.prompt_processor = PromptProcessor(model_tokenizer_path)\n",
        "\n",
        "        print(f\"Loading Whisper model: {whisper_model_name} on {self.device}\")\n",
        "        self.whisper_model = whisper.load_model(whisper_model_name, device=self.device)\n",
        "        print(\"Whisper model loaded.\")\n",
        "\n",
        "    # Renamed and adapted from the previous version\n",
        "    def create_speaker_representation(self, audio_bytes: bytes, transcript: str):\n",
        "        \"\"\"\n",
        "        Creates a v3-compatible speaker dictionary using Whisper and AudioProcessor.\n",
        "        \"\"\"\n",
        "        if not audio_bytes or not transcript:\n",
        "             print(\"Missing audio bytes or transcript in create_speaker_representation.\")\n",
        "             return None\n",
        "\n",
        "        # Whisper needs a file path, so save bytes to a temporary file\n",
        "        try:\n",
        "            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as tmp_audio_file:\n",
        "                tmp_audio_file.write(audio_bytes)\n",
        "                tmp_audio_file.flush() # Ensure data is written\n",
        "\n",
        "                # 1. Get word timings using Whisper\n",
        "                whisper_result = self.whisper_model.transcribe(tmp_audio_file.name, word_timestamps=True)\n",
        "                # Use the provided transcript for consistency, but Whisper timings\n",
        "                normalized_transcript = text_normalizations(transcript)\n",
        "\n",
        "                words_with_timings = []\n",
        "                if whisper_result and 'segments' in whisper_result:\n",
        "                    for segment in whisper_result['segments']:\n",
        "                        if 'words' in segment:\n",
        "                            for word_info in segment['words']:\n",
        "                                # Use original word casing/punctuation from Whisper's output if needed,\n",
        "                                # but strip excess whitespace for consistency.\n",
        "                                cleaned_word = word_info['word'].strip()\n",
        "                                if cleaned_word: # Ignore empty strings\n",
        "                                    words_with_timings.append({\n",
        "                                        'word': cleaned_word,\n",
        "                                        'start': float(word_info['start']),\n",
        "                                        'end': float(word_info['end'])\n",
        "                                    })\n",
        "                else:\n",
        "                    print(f\"Whisper did not return segments/words for: {transcript[:50]}...\")\n",
        "                    return None # Indicate failure\n",
        "\n",
        "                if not words_with_timings:\n",
        "                    print(f\"No word timings extracted by Whisper for: {transcript[:50]}...\")\n",
        "                    return None\n",
        "\n",
        "                # Prepare data dict for AudioProcessor\n",
        "                speaker_data_dict = {\n",
        "                    \"audio\": {\"bytes\": audio_bytes},\n",
        "                    \"text\": normalized_transcript, # Use the potentially normalized transcript\n",
        "                    \"words\": words_with_timings\n",
        "                }\n",
        "\n",
        "                # 2. Use AudioProcessor to create the speaker representation\n",
        "                v3_speaker = self.audio_processor.create_speaker_from_dict(speaker_data_dict)\n",
        "                return v3_speaker\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during speaker creation (Whisper/AudioProcessor): {e}\")\n",
        "            return None # Indicate failure\n",
        "\n",
        "\n",
        "    # --- V3 Changes: run method is now a generator ---\n",
        "    def process_dataset(self, dataset: Dataset):\n",
        "        \"\"\"\n",
        "        Processes a Hugging Face Dataset object in memory and yields training prompts.\n",
        "\n",
        "        Args:\n",
        "            dataset (Dataset): The Hugging Face dataset to process.\n",
        "                               Expected columns: 'text' (str) and 'audio' (dict with 'bytes').\n",
        "\n",
        "        Yields:\n",
        "            str: The processed training prompt string for each valid row.\n",
        "        \"\"\"\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        # Iterate directly over the dataset\n",
        "        for i, item in enumerate(tqdm(dataset, desc=\"Processing Dataset\")):\n",
        "            try:\n",
        "                # --- Adapt to your dataset's column names ---\n",
        "                transcript = item.get('text')\n",
        "                audio_info = item.get('audio')\n",
        "                # --- End Adapt ---\n",
        "\n",
        "                if not transcript or not isinstance(transcript, str):\n",
        "                    print(f\"Row {i}: Skipping due to missing or invalid 'text' column.\")\n",
        "                    skipped_count += 1\n",
        "                    continue\n",
        "\n",
        "                audio_array = audio_info['array']\n",
        "                buffer = io.BytesIO()\n",
        "                # Ensure array is float32 for common compatibility, adjust subtype if needed\n",
        "                sf.write(buffer, audio_array.astype(np.float32), audio_info['sampling_rate'], format='WAV', subtype='FLOAT')\n",
        "                buffer.seek(0)\n",
        "                audio_bytes = buffer.getvalue()\n",
        "\n",
        "                # Create speaker representation\n",
        "                speaker = self.create_speaker_representation(audio_bytes, transcript)\n",
        "\n",
        "                if speaker is None:\n",
        "                    print(f\"Row {i}: Failed to create speaker representation for text: {transcript[:50]}... Skipping.\")\n",
        "                    skipped_count += 1\n",
        "                    continue\n",
        "\n",
        "                # Get the V3 training prompt\n",
        "                prompt = self.prompt_processor.get_training_prompt(speaker)\n",
        "\n",
        "                processed_count += 1\n",
        "                yield prompt # Yield the processed prompt string\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                 print(\"Processing interrupted by user.\")\n",
        "                 break\n",
        "            except Exception as e:\n",
        "                print(f\"Row {i}: Unhandled error processing item: {e}\", exc_info=True)\n",
        "                skipped_count += 1\n",
        "                # Decide if you want to stop on errors or just skip\n",
        "                continue\n",
        "\n",
        "        print(f\"Dataset processing finished. Processed: {processed_count}, Skipped: {skipped_count}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    _MODEL_TOKENIZER_PATH = \"OuteAI/Llama-OuteTTS-1.0-1B\"\n",
        "    _WHISPER_MODEL = \"turbo\" # Or \"small.en\", \"medium.en\", \"large-v2\", etc.\n",
        "\n",
        "\n",
        "    data_processor = DataCreationV3(\n",
        "        model_tokenizer_path=_MODEL_TOKENIZER_PATH,\n",
        "        whisper_model_name=_WHISPER_MODEL\n",
        "    )\n",
        "\n",
        "    # Process the dataset and collect prompts (or process iteratively)\n",
        "    all_prompts = []\n",
        "    print(\"Starting dataset processing...\")\n",
        "    procced_dataset = data_processor.process_dataset(dataset)\n",
        "    for prompt in procced_dataset:\n",
        "        if prompt:\n",
        "             all_prompts.append({'text': prompt})\n",
        "    dataset = Dataset.from_list(all_prompts)\n",
        "    print(\"Moving Whisper model to CPU\")\n",
        "    data_processor.whisper_model.to('cpu')\n",
        "    torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apUdB40Ep6Ki"
      },
      "outputs": [],
      "source": [
        "input_text = \"Hey there my name is Elise, and I'm a speech generation model that can sound like a person.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "krYI8PrRJ6MX"
      },
      "outputs": [],
      "source": [
        "#@title Run Inference\n",
        "\n",
        "import torch\n",
        "import re\n",
        "import numpy as np\n",
        "from typing import Dict, Any\n",
        "import torchaudio.transforms as T\n",
        "from transformers import LogitsProcessor\n",
        "import transformers.generation.utils as generation_utils\n",
        "from transformers import AutoModelForCausalLM\n",
        "import re\n",
        "\n",
        "def get_audio(tokens):\n",
        "        decoded_output = tokenizer.batch_decode(tokens, skip_special_tokens=False)[0]\n",
        "        c1 = list(map(int,re.findall(r\"<\\|c1_(\\d+)\\|>\", decoded_output)))\n",
        "        c2 = list(map(int,re.findall(r\"<\\|c2_(\\d+)\\|>\", decoded_output)))\n",
        "\n",
        "        t = min(len(c1), len(c2))\n",
        "        c1 = c1[:t]\n",
        "        c2 = c2[:t]\n",
        "        output = [c1,c2]\n",
        "        if not output:\n",
        "            print(\"No audio tokens found in the output\")\n",
        "            return None\n",
        "\n",
        "        return data_processor.audio_processor.audio_codec.decode(\n",
        "            torch.tensor([output], dtype=torch.int64).to(data_processor.audio_processor.audio_codec.device)\n",
        "        )\n",
        "\n",
        "class RepetitionPenaltyLogitsProcessorPatch(LogitsProcessor):\n",
        "    def __init__(self, penalty: float):\n",
        "        penalty_last_n = 64\n",
        "        print(\"üîÑ Using patched RepetitionPenaltyLogitsProcessor -> RepetitionPenaltyLogitsProcessorPatch | penalty_last_n: {penalty_last_n}\")\n",
        "        if penalty_last_n is not None:\n",
        "            if not isinstance(penalty_last_n, int) or penalty_last_n < 0:\n",
        "                raise ValueError(f\"`penalty_last_n` has to be a non-negative integer, but is {penalty_last_n}\")\n",
        "        if not isinstance(penalty, float) or penalty <= 0:\n",
        "            raise ValueError(f\"`penalty` has to be a positive float, but is {penalty}\")\n",
        "\n",
        "        self.penalty_last_n = penalty_last_n\n",
        "        self.penalty = penalty\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids (`torch.LongTensor`):\n",
        "                Indices of input sequence tokens in the vocabulary (shape `(batch_size, sequence_length)`).\n",
        "            scores (`torch.FloatTensor`):\n",
        "                Prediction scores of a language modeling head (shape `(batch_size, vocab_size)`).\n",
        "\n",
        "        Returns:\n",
        "            `torch.FloatTensor`: The modified prediction scores.\n",
        "        \"\"\"\n",
        "        # Check if penalties should be applied\n",
        "        if self.penalty_last_n == 0 or self.penalty == 1.0:\n",
        "            return scores\n",
        "\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        vocab_size = scores.shape[-1]\n",
        "\n",
        "        # Process each batch item independently\n",
        "        for b in range(batch_size):\n",
        "            # 1. Determine the penalty window\n",
        "            start_index = max(0, seq_len - self.penalty_last_n)\n",
        "            window_indices = input_ids[b, start_index:] # Shape: (window_len,)\n",
        "\n",
        "            if window_indices.numel() == 0: # Skip if window is empty\n",
        "                continue\n",
        "\n",
        "            # 2. Find unique tokens within the window\n",
        "            tokens_in_window = set(window_indices.tolist())\n",
        "\n",
        "            # 3. Apply repetition penalty to the scores for this batch item\n",
        "            for token_id in tokens_in_window:\n",
        "                if token_id >= vocab_size:\n",
        "                    continue\n",
        "\n",
        "                logit = scores[b, token_id]\n",
        "\n",
        "                if logit <= 0:\n",
        "                    logit *= self.penalty\n",
        "                else:\n",
        "                    logit /= self.penalty\n",
        "\n",
        "                # Update the score\n",
        "                scores[b, token_id] = logit\n",
        "\n",
        "        return scores\n",
        "\n",
        "generation_utils.RepetitionPenaltyLogitsProcessor = RepetitionPenaltyLogitsProcessorPatch\n",
        "AutoModelForCausalLM.generate = generation_utils.GenerationMixin.generate\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    formated_text = \"<|text_start|>\"+input_text+\"<|text_end|>\"\n",
        "    prompt = \"\\n\".join([\n",
        "        \"<|im_start|>\",\n",
        "        formated_text,\n",
        "        \"<|audio_start|><|global_features_start|>\",\n",
        "    ])\n",
        "    with torch.inference_mode():\n",
        "        with torch.amp.autocast('cuda',dtype=model.dtype):\n",
        "          model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "          print(\"Generating token sequence...\")\n",
        "          generated_ids = model.generate(\n",
        "              **model_inputs,\n",
        "              temperature=0.4,\n",
        "              top_k=40,\n",
        "              top_p=0.9,\n",
        "              repetition_penalty=1.1,\n",
        "              min_p=0.05,\n",
        "              max_new_tokens=2048, # Limit generation length\n",
        "          )\n",
        "          print(\"Token sequence generated.\")\n",
        "\n",
        "\n",
        "    generated_ids_trimmed = generated_ids[:, model_inputs.input_ids.shape[1]:]\n",
        "    audio = get_audio(generated_ids)\n",
        "    audio = audio.cpu()\n",
        "    from IPython.display import Audio, display\n",
        "    display(Audio(audio.squeeze(0), rate=24000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXQAC8JVgqAL"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}