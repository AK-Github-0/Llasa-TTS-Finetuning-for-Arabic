{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AK-Github-0/Llasa-TTS-Finetuning-for-Arabic/blob/main/nb/Llasa_TTS_(1B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ARrtr9g35VZ"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGot6ycX35Vc"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3i_y5ph35Ve"
      },
      "source": [
        "Read our **[TTS Guide](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning)** for instructions and all our notebooks.\n",
        "\n",
        "Read our **[Qwen3 Guide](https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA8WYbef35Vg"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tm4RTeDr35Vi"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Note Llasa needs unsloth==2025.4.1 and transformers==4.48 to be stable!\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth==2025.4.1\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth==2025.4.1\n",
        "\n",
        "!pip install torchtune torchao vector_quantize_pytorch einx tiktoken xcodec2==0.1.5 --no-deps\n",
        "!pip install transformers==4.48 omegaconf\n",
        "%env UNSLOTH_DISABLE_FAST_GENERATION = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkWYsztAs9Ky"
      },
      "source": [
        "### Unsloth\n",
        "\n",
        "`FastModel` supports loading nearly any model now! This includes Vision and Text models!\n",
        "\n",
        "Thank you to [Etherl](https://huggingface.co/Etherll) for creating this notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "9003c06d-2512-4c9a-845c-b21e2e260a62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.5.6: Fast Llama patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "unsloth/llasa-1b does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any for long context!\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "    # Qwen3 new models\n",
        "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
        "    # Other very popular models!\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/Llama-3.3-70B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llasa-1B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = None, # Select None for auto detection\n",
        "    load_in_4bit = False, # Choose True for 4bit which reduces memory\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"v_proj\"],\n",
        "    lora_alpha = 128,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep  \n",
        "\n",
        "We will use the `MrDragonFox/Elise`, which is designed for training TTS models. Ensure that your dataset follows the required format:\n",
        "**text, audio**. You can modify this section to accommodate your own dataset, but maintaining the correct structure is essential for optimal training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"MohamedRashad/SCC22\", split = \"test\")\n",
        "OUTPUT_DIR = 'processed_data_memmap'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK94B-Pfioto",
        "outputId": "94caf091-cf60-4498-dec9-99995f51c232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type xcodec2 to instantiate a model of type xcodec. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the model checkpoint at HKUST-Audio/xcodec2 were not used when initializing XCodec2Model: ['CodecEnc.conv_blocks.1.block.0.block.0.act.beta', 'CodecEnc.conv_blocks.1.block.0.block.2.act.beta', 'CodecEnc.conv_blocks.1.block.1.block.0.act.beta', 'CodecEnc.conv_blocks.1.block.1.block.2.act.beta', 'CodecEnc.conv_blocks.1.block.2.block.0.act.beta', 'CodecEnc.conv_blocks.1.block.2.block.2.act.beta', 'CodecEnc.conv_blocks.1.block.3.act.beta', 'CodecEnc.conv_blocks.2.block.0.block.0.act.beta', 'CodecEnc.conv_blocks.2.block.0.block.2.act.beta', 'CodecEnc.conv_blocks.2.block.1.block.0.act.beta', 'CodecEnc.conv_blocks.2.block.1.block.2.act.beta', 'CodecEnc.conv_blocks.2.block.2.block.0.act.beta', 'CodecEnc.conv_blocks.2.block.2.block.2.act.beta', 'CodecEnc.conv_blocks.2.block.3.act.beta', 'CodecEnc.conv_blocks.3.block.0.block.0.act.beta', 'CodecEnc.conv_blocks.3.block.0.block.2.act.beta', 'CodecEnc.conv_blocks.3.block.1.block.0.act.beta', 'CodecEnc.conv_blocks.3.block.1.block.2.act.beta', 'CodecEnc.conv_blocks.3.block.2.block.0.act.beta', 'CodecEnc.conv_blocks.3.block.2.block.2.act.beta', 'CodecEnc.conv_blocks.3.block.3.act.beta', 'CodecEnc.conv_blocks.4.block.0.block.0.act.beta', 'CodecEnc.conv_blocks.4.block.0.block.2.act.beta', 'CodecEnc.conv_blocks.4.block.1.block.0.act.beta', 'CodecEnc.conv_blocks.4.block.1.block.2.act.beta', 'CodecEnc.conv_blocks.4.block.2.block.0.act.beta', 'CodecEnc.conv_blocks.4.block.2.block.2.act.beta', 'CodecEnc.conv_blocks.4.block.3.act.beta', 'CodecEnc.conv_blocks.5.block.0.block.0.act.beta', 'CodecEnc.conv_blocks.5.block.0.block.2.act.beta', 'CodecEnc.conv_blocks.5.block.1.block.0.act.beta', 'CodecEnc.conv_blocks.5.block.1.block.2.act.beta', 'CodecEnc.conv_blocks.5.block.2.block.0.act.beta', 'CodecEnc.conv_blocks.5.block.2.block.2.act.beta', 'CodecEnc.conv_blocks.5.block.3.act.beta', 'CodecEnc.conv_final_block.0.act.beta']\n",
            "- This IS expected if you are initializing XCodec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XCodec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XCodec2Model were not initialized from the model checkpoint at HKUST-Audio/xcodec2 and are newly initialized: ['CodecEnc.conv_blocks.1.block.0.block.0.act.bias', 'CodecEnc.conv_blocks.1.block.0.block.2.act.bias', 'CodecEnc.conv_blocks.1.block.1.block.0.act.bias', 'CodecEnc.conv_blocks.1.block.1.block.2.act.bias', 'CodecEnc.conv_blocks.1.block.2.block.0.act.bias', 'CodecEnc.conv_blocks.1.block.2.block.2.act.bias', 'CodecEnc.conv_blocks.1.block.3.act.bias', 'CodecEnc.conv_blocks.2.block.0.block.0.act.bias', 'CodecEnc.conv_blocks.2.block.0.block.2.act.bias', 'CodecEnc.conv_blocks.2.block.1.block.0.act.bias', 'CodecEnc.conv_blocks.2.block.1.block.2.act.bias', 'CodecEnc.conv_blocks.2.block.2.block.0.act.bias', 'CodecEnc.conv_blocks.2.block.2.block.2.act.bias', 'CodecEnc.conv_blocks.2.block.3.act.bias', 'CodecEnc.conv_blocks.3.block.0.block.0.act.bias', 'CodecEnc.conv_blocks.3.block.0.block.2.act.bias', 'CodecEnc.conv_blocks.3.block.1.block.0.act.bias', 'CodecEnc.conv_blocks.3.block.1.block.2.act.bias', 'CodecEnc.conv_blocks.3.block.2.block.0.act.bias', 'CodecEnc.conv_blocks.3.block.2.block.2.act.bias', 'CodecEnc.conv_blocks.3.block.3.act.bias', 'CodecEnc.conv_blocks.4.block.0.block.0.act.bias', 'CodecEnc.conv_blocks.4.block.0.block.2.act.bias', 'CodecEnc.conv_blocks.4.block.1.block.0.act.bias', 'CodecEnc.conv_blocks.4.block.1.block.2.act.bias', 'CodecEnc.conv_blocks.4.block.2.block.0.act.bias', 'CodecEnc.conv_blocks.4.block.2.block.2.act.bias', 'CodecEnc.conv_blocks.4.block.3.act.bias', 'CodecEnc.conv_blocks.5.block.0.block.0.act.bias', 'CodecEnc.conv_blocks.5.block.0.block.2.act.bias', 'CodecEnc.conv_blocks.5.block.1.block.0.act.bias', 'CodecEnc.conv_blocks.5.block.1.block.2.act.bias', 'CodecEnc.conv_blocks.5.block.2.block.0.act.bias', 'CodecEnc.conv_blocks.5.block.2.block.2.act.bias', 'CodecEnc.conv_blocks.5.block.3.act.bias', 'CodecEnc.conv_final_block.0.act.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Processing:  56%|█████▌    | 1831/3296 [09:29<07:49,  3.12it/s]"
          ]
        }
      ],
      "source": [
        "#@title Tokenization Function\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torchaudio\n",
        "from transformers import AutoTokenizer\n",
        "from xcodec2.modeling_xcodec2 import XCodec2Model\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "XCODEC2_MODEL_NAME = \"HKUST-Audio/xcodec2\"\n",
        "SAMPLE_RATE = 16000\n",
        "DEVICE = \"cuda\"\n",
        "\n",
        "def preprocess_and_save(\n",
        "    dataset,\n",
        "    output_dir: str,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    codec_model: XCodec2Model,\n",
        "    sample_rate: int = 16000,\n",
        "    max_length: int = 2048,\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "):\n",
        "    codec_model = codec_model.to(device).eval()\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    memmap_path = os.path.join(output_dir, f\"input_ids.memmap\")\n",
        "    shape_path = os.path.join(output_dir, f\"input_ids_shape.npy\")\n",
        "    num_samples = len(dataset)\n",
        "    shape = (num_samples, max_length)\n",
        "    try:\n",
        "        arr = np.memmap(memmap_path, dtype=np.int32, mode='w+', shape=shape)\n",
        "    except Exception as e:\n",
        "        raise e\n",
        "    valid_sequences_count = 0\n",
        "    skipped_count = 0\n",
        "    for idx, example in tqdm(enumerate(dataset), total=num_samples, desc=f\"Processing\"):\n",
        "        try:\n",
        "            if 'text' not in example or example['text'] is None:\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "            text = f\"<|TEXT_UNDERSTANDING_START|>{example['text']}<|TEXT_UNDERSTANDING_END|>\"\n",
        "            text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "            if \"audio\" not in example or \"array\" not in example[\"audio\"] or \"sampling_rate\" not in example[\"audio\"] or example[\"audio\"][\"array\"] is None:\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "            waveform = torch.tensor(example[\"audio\"][\"array\"]).float()\n",
        "            original_sr = example[\"audio\"][\"sampling_rate\"]\n",
        "            if original_sr != sample_rate:\n",
        "              waveform = torchaudio.functional.resample(waveform, original_sr, sample_rate)\n",
        "\n",
        "            original_shape_after_resample = waveform.shape\n",
        "            waveform = waveform.squeeze()\n",
        "            if waveform.dim() == 0:\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "            elif waveform.dim() > 1:\n",
        "                waveform = waveform[0]\n",
        "                if waveform.dim() != 1:\n",
        "                    skipped_count += 1\n",
        "                    continue\n",
        "            final_waveform = waveform.unsqueeze(0).to(device)\n",
        "            speech_codes = None\n",
        "            with torch.inference_mode():\n",
        "                speech_codes_raw = codec_model.encode_code(final_waveform)\n",
        "                speech_codes = speech_codes_raw[0][0]\n",
        "\n",
        "            codes_np = speech_codes.cpu().numpy()\n",
        "            speech_token_ids = [f\"<|s_{code}|>\" for code in codes_np]\n",
        "            speech_token_ids = tokenizer.convert_tokens_to_ids(speech_token_ids)\n",
        "            speech_ids = (\n",
        "                [tokenizer.convert_tokens_to_ids(\"<|SPEECH_GENERATION_START|>\")]\n",
        "                + speech_token_ids\n",
        "                + [tokenizer.convert_tokens_to_ids(\"<|SPEECH_GENERATION_END|>\")]\n",
        "            )\n",
        "            max_text_space = max_length - len(speech_ids)\n",
        "            if max_text_space < 0:\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "            truncated_text_ids = text_ids[:max_text_space]\n",
        "            combined_sequence = truncated_text_ids + speech_ids\n",
        "            padding_length = max_length - len(combined_sequence)\n",
        "            final_sequence = combined_sequence + [tokenizer.pad_token_id] * padding_length\n",
        "            final_sequence = final_sequence[:max_length]\n",
        "            arr[valid_sequences_count] = np.array(final_sequence, dtype=np.int32)\n",
        "            valid_sequences_count += 1\n",
        "        except Exception as e:\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "        arr.flush()\n",
        "        actual_shape = (valid_sequences_count, max_length)\n",
        "        np.save(shape_path, np.array(actual_shape))\n",
        "\n",
        "\n",
        "class MemmapTTSDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_path: str,\n",
        "        tokenizer: AutoTokenizer,\n",
        "        max_length: int = 2048,\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        if self.tokenizer.pad_token_id is None:\n",
        "             if self.tokenizer.eos_token_id is not None:\n",
        "                 self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "             else:\n",
        "                 raise ValueError(\"Tokenizer passed to Dataset must have pad_token_id set.\")\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.ignore_index = -100\n",
        "\n",
        "        memmap_file = os.path.join(data_path, f'input_ids.memmap')\n",
        "        shape_file = os.path.join(data_path, f'input_ids_shape.npy')\n",
        "\n",
        "        if not os.path.exists(memmap_file) or not os.path.exists(shape_file):\n",
        "             raise FileNotFoundError(f\"Required files not found  in {data_path}\")\n",
        "\n",
        "        self.shape = tuple(np.load(shape_file))\n",
        "        if not self.shape or len(self.shape) != 2 or self.shape[0] == 0:\n",
        "             self.length = 0\n",
        "             self.memmap_data = None\n",
        "        else:\n",
        "             self.memmap_data = np.memmap(memmap_file, dtype='int32', mode='r', shape=self.shape)\n",
        "             self.length = self.shape[0]\n",
        "\n",
        "        try:\n",
        "            self.speech_generation_start_id = tokenizer.convert_tokens_to_ids('<|SPEECH_GENERATION_START|>')\n",
        "            self.speech_generation_end_id = tokenizer.convert_tokens_to_ids('<|SPEECH_GENERATION_END|>')\n",
        "            self.text_understanding_start_id = tokenizer.convert_tokens_to_ids('<|TEXT_UNDERSTANDING_START|>')\n",
        "            self.text_understanding_end_id = tokenizer.convert_tokens_to_ids('<|TEXT_UNDERSTANDING_END|>')\n",
        "            assert isinstance(self.pad_token_id, int)\n",
        "        except Exception as token_err:\n",
        "            raise ValueError(f\"Tokenizer is missing required special tokens or pad_token_id. Error: {token_err}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def replace_tagged_token(self, token_list, target_token_id, new_sequence_ids):\n",
        "        if isinstance(new_sequence_ids, torch.Tensor):\n",
        "            new_sequence_ids = new_sequence_ids.tolist()\n",
        "        try:\n",
        "            idx = token_list.index(target_token_id)\n",
        "            return token_list[:idx] + new_sequence_ids + token_list[idx+1:]\n",
        "        except ValueError:\n",
        "            return token_list\n",
        "\n",
        "    def pad_sequence_torch(self, sequence, max_length, value=0):\n",
        "        current_len = len(sequence)\n",
        "        if current_len >= max_length:\n",
        "            return sequence[:max_length]\n",
        "        else:\n",
        "            padding_size = max_length - current_len\n",
        "            padding = torch.full((padding_size,), value, dtype=sequence.dtype, device=sequence.device)\n",
        "            return torch.cat([sequence, padding], dim=0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.memmap_data is None or idx >= self.length:\n",
        "            raise IndexError(f\"Index out of bounds (length={self.length}).\")\n",
        "\n",
        "        raw_input_ids_np = self.memmap_data[idx]\n",
        "        input_ids_tensor_raw = torch.tensor(raw_input_ids_np, dtype=torch.long)\n",
        "        input_ids = None\n",
        "        speech_gen_idx_in_final = -1\n",
        "\n",
        "        try:\n",
        "            text_start_idx = (input_ids_tensor_raw == self.text_understanding_start_id).nonzero(as_tuple=True)[0][0].item()\n",
        "            text_end_idx = (input_ids_tensor_raw == self.text_understanding_end_id).nonzero(as_tuple=True)[0][0].item()\n",
        "            speech_start_idx = (input_ids_tensor_raw == self.speech_generation_start_id).nonzero(as_tuple=True)[0][0].item()\n",
        "\n",
        "            speech_end_marker_indices = (input_ids_tensor_raw == self.speech_generation_end_id).nonzero(as_tuple=True)[0]\n",
        "            pad_start_indices = (input_ids_tensor_raw == self.pad_token_id).nonzero(as_tuple=True)[0]\n",
        "\n",
        "            if len(speech_end_marker_indices) > 0:\n",
        "                 speech_end_idx = speech_end_marker_indices[0].item()\n",
        "            elif len(pad_start_indices) > 0:\n",
        "                 speech_end_idx = pad_start_indices[0].item() - 1\n",
        "            else:\n",
        "                 speech_end_idx = len(input_ids_tensor_raw) - 1\n",
        "\n",
        "            if not (text_start_idx < text_end_idx < speech_start_idx < speech_end_idx < len(input_ids_tensor_raw)):\n",
        "                 raise ValueError(\"Parsed indices are invalid or out of order\")\n",
        "\n",
        "            original_text_sequence = input_ids_tensor_raw[:speech_start_idx]\n",
        "            original_speech_sequence_with_markers = input_ids_tensor_raw[speech_start_idx : speech_end_idx +1]\n",
        "            chat = [\n",
        "                {\"role\": \"user\", \"content\": f\"Convert the text to speech:<|TEXT_UNDERSTANDING_START|>\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"<|SPEECH_GENERATION_START|>\"}\n",
        "            ]\n",
        "\n",
        "            try:\n",
        "                 import inspect\n",
        "                 sig = inspect.signature(self.tokenizer.apply_chat_template)\n",
        "                 if 'add_generation_prompt' in sig.parameters:\n",
        "                     templated_ids_list = self.tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=False)\n",
        "                 else:\n",
        "                     templated_ids_list = self.tokenizer.apply_chat_template(chat, tokenize=True)\n",
        "\n",
        "                 final_ids_list = self.replace_tagged_token(templated_ids_list, self.text_understanding_start_id, original_text_sequence.tolist())\n",
        "                 final_ids_list = self.replace_tagged_token(final_ids_list, self.speech_generation_start_id, original_speech_sequence_with_markers.tolist())\n",
        "                 input_ids = torch.tensor(final_ids_list, dtype=torch.long)\n",
        "\n",
        "                 try:\n",
        "                     speech_gen_idx_in_final = (input_ids == self.speech_generation_start_id).nonzero(as_tuple=True)[0][0].item()\n",
        "                 except IndexError:\n",
        "                      speech_gen_idx_in_final = -1\n",
        "            except Exception:\n",
        "                 input_ids = input_ids_tensor_raw\n",
        "                 try:\n",
        "                     speech_gen_idx_in_final = (input_ids == self.speech_generation_start_id).nonzero(as_tuple=True)[0][0].item()\n",
        "                 except IndexError:\n",
        "                     speech_gen_idx_in_final = -1\n",
        "\n",
        "        except Exception:\n",
        "            input_ids = input_ids_tensor_raw\n",
        "            try:\n",
        "                speech_gen_idx_in_final = (input_ids == self.speech_generation_start_id).nonzero(as_tuple=True)[0][0].item()\n",
        "            except IndexError:\n",
        "                speech_gen_idx_in_final = -1\n",
        "\n",
        "        if input_ids is None:\n",
        "            input_ids = input_ids_tensor_raw\n",
        "            try:\n",
        "                speech_gen_idx_in_final = (input_ids == self.speech_generation_start_id).nonzero(as_tuple=True)[0][0].item()\n",
        "            except IndexError:\n",
        "                speech_gen_idx_in_final = -1\n",
        "\n",
        "        labels = torch.full_like(input_ids, self.ignore_index)\n",
        "        if speech_gen_idx_in_final != -1 and speech_gen_idx_in_final < len(input_ids):\n",
        "             labels[speech_gen_idx_in_final:] = input_ids[speech_gen_idx_in_final:].clone()\n",
        "\n",
        "        attention_mask = (input_ids != self.pad_token_id).long()\n",
        "        labels[input_ids == self.pad_token_id] = self.ignore_index\n",
        "\n",
        "        input_ids = self.pad_sequence_torch(input_ids, self.max_length, value=self.pad_token_id)\n",
        "        attention_mask = self.pad_sequence_torch(attention_mask, self.max_length, value=0)\n",
        "        labels = self.pad_sequence_torch(labels, self.max_length, value=self.ignore_index)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'labels': labels,\n",
        "            'attention_mask': attention_mask\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    codec_model = XCodec2Model.from_pretrained(XCODEC2_MODEL_NAME)\n",
        "\n",
        "except Exception as e:\n",
        "    raise f\"ERROR loading XCodec2 model: {e}.\"\n",
        "\n",
        "preprocess_and_save(\n",
        "        dataset=dataset,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        tokenizer=tokenizer,\n",
        "        codec_model=codec_model,\n",
        "        sample_rate=SAMPLE_RATE,\n",
        "        max_length=max_seq_length,\n",
        "        device=DEVICE\n",
        "    )\n",
        "try:\n",
        "    train_dataset = MemmapTTSDataset(\n",
        "        data_path=OUTPUT_DIR,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=max_seq_length\n",
        "     )\n",
        "    print(f\"Dataset loaded for split 'train'. Number of samples: {len(train_dataset)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing dataset: {e}\")\n",
        "print(\"Moving XCodec2 model to cpu\")\n",
        "codec_model.to('cpu')\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface  `Trainer`! More docs here: [Transformers docs](https://huggingface.co/docs/transformers/main_classes/trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        #num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 5e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgP6HUMgeTqP"
      },
      "outputs": [],
      "source": [
        "input_text = \"اربك تكست هو اول موقع يسمح لزواره الكرام بتحويل الكتابة العربي الى كتابة مفهومة من قبل اغلب برامج التصميم مثل الفوتوشوب و الافترايفكتس و البريمير و الافد\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apUdB40Ep6Ki"
      },
      "outputs": [],
      "source": [
        "#@title Run Inference\n",
        "import soundfile as sf\n",
        "\n",
        "from IPython.display import display, Audio\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def ids_to_speech_tokens(speech_ids):\n",
        "\n",
        "    speech_tokens_str = []\n",
        "    for speech_id in speech_ids:\n",
        "        speech_tokens_str.append(f\"<|s_{speech_id}|>\")\n",
        "    return speech_tokens_str\n",
        "\n",
        "def extract_speech_ids(speech_tokens_str):\n",
        "\n",
        "    speech_ids = []\n",
        "    for token_str in speech_tokens_str:\n",
        "        if token_str.startswith('<|s_') and token_str.endswith('|>'):\n",
        "            num_str = token_str[4:-2]\n",
        "\n",
        "            num = int(num_str)\n",
        "            speech_ids.append(num)\n",
        "        else:\n",
        "            print(f\"Unexpected token: {token_str}\")\n",
        "    return speech_ids\n",
        "\n",
        "#TTS start!\n",
        "with torch.inference_mode():\n",
        "    with torch.amp.autocast('cuda',dtype=model.dtype):\n",
        "        formatted_text = f\"<|TEXT_UNDERSTANDING_START|>{input_text}<|TEXT_UNDERSTANDING_END|>\"\n",
        "\n",
        "        # Tokenize the text\n",
        "        chat = [\n",
        "            {\"role\": \"user\", \"content\": \"Convert the text to speech:\" + formatted_text},\n",
        "            {\"role\": \"assistant\", \"content\": \"<|SPEECH_GENERATION_START|>\"}\n",
        "        ]\n",
        "\n",
        "        input_ids = tokenizer.apply_chat_template(\n",
        "            chat,\n",
        "            tokenize=True,\n",
        "            return_tensors='pt',\n",
        "            continue_final_message=True\n",
        "        )\n",
        "        input_ids = input_ids.to('cuda')\n",
        "\n",
        "        speech_end_id = tokenizer.convert_tokens_to_ids('<|SPEECH_GENERATION_END|>')\n",
        "\n",
        "        # Generate the speech autoregressively\n",
        "        outputs = model.generate(\n",
        "            input_ids,\n",
        "            max_length=2048,  # We trained our model with a max length of 2048\n",
        "            eos_token_id= speech_end_id ,\n",
        "            do_sample=True,\n",
        "            top_p=1.2,           #  Adjusts the diversity of generated content\n",
        "            temperature=1.2,   #  Controls randomness in output\n",
        "        )\n",
        "    # Extract the speech tokens\n",
        "    generated_ids = outputs[0][input_ids.shape[1]:-1]\n",
        "\n",
        "    speech_tokens = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Convert  token <|s_23456|> to int 23456\n",
        "    speech_tokens = extract_speech_ids(speech_tokens)\n",
        "\n",
        "    speech_tokens = torch.tensor(speech_tokens).cpu().unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    # Decode the speech tokens to speech waveform\n",
        "    gen_wav = codec_model.decode_code(speech_tokens)\n",
        "\n",
        "sf.write(\"output.wav\", gen_wav[0, 0, :].cpu().numpy(), 16000)\n",
        "\n",
        "display(Audio(gen_wav[0, 0, :].cpu().numpy(), rate=16000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BKcejRlhRNK"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31011,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}